import os
from typing import Generator, Optional, Sequence, Callable
from functools import partial

import numpy as np
from scipy.io import arff


# def _multisine(x, coeff) -> float:
#     return sum([
#         coeff[i, 0] * np.sin(coeff[i, 1]*x + coeff[i, 2])
#         for i in range(len(coeff))
#     ])
def _multisine(x, coeff) -> float:
    ret = 0.
    for i in range(len(coeff)):
        if callable(coeff[i, 1]):
            f = coeff[i,1](x)
        else:
            f = coeff[i, 1]
        ret += coeff[i, 0] * np.sin(f*x + coeff[i, 2])
    return ret


def multisine(
    train_size: int = 100,
    test_size: int = 1000,
    length: int = 100,
    n_classes: int = 2,
    used_sines: int = 3,
    coefficients: Optional[np.ndarray] = None,
    noise: Optional[Callable[[], float]] = None,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Generates a time series dataset based on a linear concatenation
    of sine functions with random parameters for period length and more.
    These concatenations are the models for each class and a single
    sample is generated by adding a normally distributed error to the
    time series.

    Args:
        train_size (int, optional): Number of training examples. Each
            class will have approximately the same number of examples.
            Defaults to 100.
        test_size (int, optional): Number of testing examples. Each
            class will have approximately the same number of examples.
            Defaults to 1000.
        n_classes (int, optional): Number of classes to generate.
            Defaults to 2.
        length (int, optional): Length of each time series. Defaults to
            100.
        used_sines (int, optional): Number of sine functions with random
            parameters used for the generation of the dataset. Defaults
            to 5.
        coefficients (np.ndarray, optional): The coefficients of the
            sine functions as a numpy array of shape
            ``(n_classes, used_sines, 3)``. The third dimension
            describes the properties of a single sine function:
            ``[amplitude, frequency, phaseshift]``. Defaults to random
            coefficients.
        noise (callable, optional): Callable that returns a random noise
            float value. If none is given, normally distributed values
            with mean 0 and standard deviation 0.5 are used.

    Returns:
        tuple of numpy arrays: A dataset of time series in the order
            ``(X_train, y_train, X_test, y_test)``.
    """
    train_size_per_class = train_size // n_classes
    train_sizes = [train_size_per_class for i in range(n_classes)]
    remain = train_size - train_size_per_class * n_classes
    while remain > 0:
        train_sizes[remain % n_classes] += 1
        remain -= 1

    test_size_per_class = test_size // n_classes
    test_sizes = [test_size_per_class for i in range(n_classes)]
    remain = test_size - test_size_per_class * n_classes
    while remain > 0:
        test_sizes[remain % n_classes] += 1
        remain -= 1

    x_range = np.linspace(0, 2*np.pi, num=length)
    if coefficients is None:
        coefficients = 2 * np.random.rand(n_classes, used_sines, 3)

    models = []
    for i in range(n_classes):
        models.append(
            np.vectorize(lambda x: _multisine(x, coefficients[i]))(x_range)
        )

    X_train = np.zeros((train_size, length))
    X_test = np.zeros((test_size, length))
    y_train = np.zeros(train_size)
    y_test = np.zeros(test_size)

    s = 0
    for i, n_i in enumerate(train_sizes):
        for j in range(n_i):
            X_train[s+j, :] = models[i]
            if noise is None:
                a = np.random.normal(0, 0.5, length)
                X_train[s+j, :] += a
            else:
                X_train[s+j, :] += np.array([noise() for _ in range(length)])
            y_train[s+j] = i
        s += n_i

    s = 0
    for i, n_i in enumerate(test_sizes):
        for j in range(n_i):
            X_test[s+j, :] = models[i]
            if noise is None:
                X_test[s+j, :] += np.random.normal(0, 0.5, length)
            else:
                X_test[s+j, :] += np.array([noise() for _ in range(length)])
            y_test[s+j] = i
        s += n_i

    X_train = np.expand_dims(X_train, axis=1)
    X_test = np.expand_dims(X_test, axis=1)

    return X_train, y_train, X_test, y_test


def replace_nan(X: np.ndarray, value: Optional[float] = None) -> np.ndarray:
    """Replaces all NaN values in the given time series dataset.

    Args:
        X (np.ndarray): Three dimensional array containing multivariate
            time series.
        value (float, optional): If a value is given, all NaNs are
            replaced by this value. If nothing is specified, a missing
            value is replaced by the last observed value in the time
            series. If a value at time step zero is missing, it will be
            set to 0.
    """
    if value is not None:
        return np.nan_to_num(X, nan=value)
    nan_places = np.argwhere(np.isnan(X))
    out = X.copy()
    for index in nan_places:
        if index[2] == 0:
            out[index[0], index[1], index[2]] = 0
            continue
        out[index[0], index[1], index[2]] = out[index[0], index[1], index[2]-1]
    return out


def load(
    path: str,
    univariate: bool = True,
    cache: bool = True,
    keep_nan: bool = False,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Returns a time series dataset that is formatted as a .txt file
    and readable with numpy.

    Args:
        path (str): The path to a folder that contains two files
            '``D``_TEST.txt' and '``D``_TRAIN.txt' where ``D`` is the
            name of the folder.
        univariate (bool, optional): If ``True``, the function expects
            the data to be univariate and readable as ".txt" with numpy.
            Else, it searches for multivariate ".arff" files to read.
            Defaults to True.
        cache (bool, optional): If set to ``True``, use cache if exists
            (as .npy file) or create it if it doesn't. Setting this
            option to ``False`` always reads the .arff files. Defaults
            to True.
        keep_nan (bool, optional): If set to ``False``, all NaN values
            in the time series dataset will be replaced according to the
            method :meth:`replace_nan`. Defaults to False.

    Returns:
        tuple of numpy arrays: A dataset of time series in the order
            ``(X_train, y_train, X_test, y_test)``.
    """
    if univariate:
        if path.endswith("/"):
            path = os.path.dirname(path)
        dataset = os.path.basename(path)
        delim = None
        with open(f"{path}/{dataset}_TRAIN.txt") as f:
            if "," in f.readline():
                delim = ","
        train_raw = np.loadtxt(f"{path}/{dataset}_TRAIN.txt", delimiter=delim)
        test_raw = np.loadtxt(f"{path}/{dataset}_TEST.txt", delimiter=delim)
        X_train = train_raw[:, 1:].astype(np.float64)
        y_train = train_raw[:, 0].astype(np.int32)
        X_test = test_raw[:, 1:].astype(np.float64)
        y_test = test_raw[:, 0].astype(np.int32)

        X_train = np.expand_dims(X_train, axis=1)
        X_test = np.expand_dims(X_test, axis=1)
        if not keep_nan:
            X_train = replace_nan(X_train)
            X_test = replace_nan(X_test)

        return X_train, y_train, X_test, y_test

    name = os.path.basename(os.path.normpath(path))

    if cache and os.path.isfile(os.path.join(path, name)+"_XTRAIN.npy"):
        X_train = np.load(os.path.join(path, name)+"_XTRAIN.npy")
        y_train = np.load(os.path.join(path, name)+"_yTRAIN.npy")
        X_test = np.load(os.path.join(path, name)+"_XTEST.npy")
        y_test = np.load(os.path.join(path, name)+"_yTEST.npy")
    else:
        train, _ = arff.loadarff(
            open(os.path.join(path, name) + "_TRAIN.arff", "r",
                 encoding="utf8"))
        test, _ = arff.loadarff(
            open(os.path.join(path, name) + "_TEST.arff", "r",
                 encoding="utf8"))

        X_train = np.zeros(
            (
                len(train),
                len(train[0].tolist()[0]),
                len(train[0].tolist()[0][0]),
            ),
            dtype=np.float64
        )
        y_train = np.zeros((X_train.shape[0],), dtype=np.int32)

        X_test = np.zeros(
            (
                len(test),
                len(test[0].tolist()[0]),
                len(test[0].tolist()[0][0]),
            ),
            dtype=np.float64
        )
        y_test = np.zeros((X_test.shape[0],), dtype=np.int32)

        ys = []
        for m in range(X_train.shape[0]):
            X_train[m, :, :] = train[m][0].tolist()
            if train[m][1] not in ys:
                ys.append(train[m][1])
        for m in range(X_test.shape[0]):
            X_test[m, :, :] = test[m][0].tolist()
            if test[m][1] not in ys:
                ys.append(test[m][1])

        ylabels = {ys[i]: i for i in range(len(ys))}
        for m in range(X_train.shape[0]):
            y_train[m] = ylabels[train[m][1]]
        for m in range(X_test.shape[0]):
            y_test[m] = ylabels[test[m][1]]

        if cache:
            np.save(os.path.join(path, name)+"_XTRAIN",
                    X_train)
            np.save(os.path.join(path, name)+"_yTRAIN",
                    y_train)
            np.save(os.path.join(path, name)+"_XTEST",
                    X_test)
            np.save(os.path.join(path, name)+"_yTEST",
                    y_test)

    if not keep_nan:
        X_train = replace_nan(X_train)
        X_test = replace_nan(X_test)

    return X_train, y_train, X_test, y_test


def load_all(
    path: str,
    univariate: bool = True,
    cache: bool = True,
    keep_nan: bool = False,
    datasets: Optional[Sequence[str]] = None,
) -> Generator[
    tuple[str, np.ndarray, np.ndarray, np.ndarray, np.ndarray], None, None
]:
    """Finds time series datasets in the specified directory and returns
    a generator over tuples
    ``(dataset_name, X_train, y_train, X_test, y_test)``.

    Args:
        path (str): Path of a directory that contains folders
            structured like the datasets you get from
            timeseriesclassification.com.
        names (sequence of str): List of dataset names. Only
            the datasets in this list will be collected. Defaults to
            all datasets in the given path.
        cache (bool, optional): If set to ``True``, use cache if exists
            (as .npy file) or create it if it doesn't. This argument is
            directly passed to :meth:`load`. Defaults to True.
        keep_nan (bool, optional): If set to ``False``, all NaN values
            in the time series datasets will be replaced. This argument
            is directly passed to :meth:`load`. Defaults to False.
        univariate (bool, optional): Whether the data in ``path`` is
            assumed to be univariate or multivariate. Defaults to
            univariate data.
    """
    for folder in sorted(os.listdir(path)):
        if os.path.isdir(os.path.join(path, folder)):
            if datasets is None or folder in datasets:
                yield (folder, ) + load(
                        os.path.join(path, folder),
                        cache=cache,
                        keep_nan=keep_nan,
                        univariate=univariate,
                    )


def implant_stuttering(
    X: np.ndarray,
    stutter_length: float = 0.1,
) -> np.ndarray:
    """Implants some 'stuttering' to a given array of time series.
    In each time series a value at random time steps will be repeated
    consecutively a random number of times.

    Args:
        X (np.ndarray): Three dimensional array containing multivariate
            time series.
        stutter_length (float, optional): Proportional number of time
            steps each time series will be lengthened and where
            stuttering occurs. This value is a float > 0. The actual
            number of time steps will be computed by
            ``stutter_length * time_series_length``. Defaults to
            ``0.1``.

    Returns:
        np.ndarray: Time series with implanted stuttering.
    """
    additional_length = int(stutter_length * X.shape[2])
    X_new = np.zeros((X.shape[0],
                      X.shape[1],
                      X.shape[2] + additional_length))
    X_new[:, :, :X.shape[2]] = X[:, :, :]
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            # length already added to the time series
            lengthened = 0
            # index of last value created for stuttering
            prop_index = 0
            while lengthened < additional_length:
                # if X.shape[2] <= prop_index < X.shape[2] + additional_length:
                #     X_new[i, j, prop_index:] = X[i, j, -1]
                #     break
                # choose index and length of stuttering randomly
                stlength = np.random.randint(1, additional_length-lengthened+1)
                stindex = np.random.randint(prop_index + 1,
                                            X.shape[2] + additional_length)
                # if stindex is at last value of X
                if stindex >= (X.shape[2]+lengthened-1):
                    X_new[i, j, X.shape[2]+lengthened-1:] = X[i, j, -1]
                    break
                # shift values where stuttering will be implanted
                start = stindex + 1
                length = X.shape[2] - (start-lengthened)
                to = stindex + stlength + 1
                X_new[i, j, to:to+length] = X_new[i, j, start:start+length]
                # implant stuttering
                X_new[i, j, stindex+1:stindex+stlength+1] = \
                    X_new[i, j, stindex]
                lengthened += stlength
                prop_index = stindex+stlength
    return X_new


def lengthen(X: np.ndarray, length: float = 0.1) -> np.ndarray:
    """Lengthens each time series in the given array.

    Args:
        X (np.ndarray): Three dimensional array containing multivariate
            time series.
        length (float, optional): Proportional length that will be added
            to each time series as a float > 0. Defaults to ``0.1``.

    Returns:
        np.ndarray: Lengthened time series.
    """
    additional_length = int(length * X.shape[2])
    X_new = np.zeros((X.shape[0],
                      X.shape[1],
                      X.shape[2] + additional_length))
    X_new[:, :, :X.shape[2]] = X
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            X_new[i, j, X.shape[2]:] = X[i, j, -1]
    return X_new
